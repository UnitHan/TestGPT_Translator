# Ollama ë¡œì»¬ LLM í†µí•© ê°€ì´ë“œ

TestGPT TC Translatorì— Ollama ë¡œì»¬ LLM ì§€ì›ì„ ì¶”ê°€í•˜ê¸° ìœ„í•œ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [Ollama ì„¤ì¹˜ ë° ì„¤ì •](#ollama-ì„¤ì¹˜-ë°-ì„¤ì •)
3. [ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ](#ëª¨ë¸-ì„ íƒ-ê°€ì´ë“œ)
4. [í”„ë¡œê·¸ë¨ í†µí•© êµ¬í˜„](#í”„ë¡œê·¸ë¨-í†µí•©-êµ¬í˜„)
5. [ì„¤ì • í™”ë©´ êµ¬í˜„](#ì„¤ì •-í™”ë©´-êµ¬í˜„)
6. [ë°°í¬ ë° ì‚¬ìš©ì ê°€ì´ë“œ](#ë°°í¬-ë°-ì‚¬ìš©ì-ê°€ì´ë“œ)
7. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## ê°œìš”

### ì™œ Ollama ë¡œì»¬ LLMì´ í•„ìš”í•œê°€?

- **ë³´ì•ˆ**: ë¯¼ê°í•œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ë°ì´í„°ë¥¼ ì™¸ë¶€ ë„¤íŠ¸ì›Œí¬ë¡œ ì „ì†¡í•˜ì§€ ì•ŠìŒ
- **í”„ë¼ì´ë²„ì‹œ**: íšŒì‚¬ ë³´ì•ˆíŒ€ì˜ ë„¤íŠ¸ì›Œí¬ ëª¨ë‹ˆí„°ë§ì—ì„œ ììœ ë¡œì›€
- **ì˜¤í”„ë¼ì¸**: ì¸í„°ë„· ì—†ì´ ë²ˆì—­ ê°€ëŠ¥
- **ë¹„ìš©**: API ë¹„ìš© ì—†ìŒ

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

**ìµœì†Œ ì‚¬ì–‘:**
- RAM: 8GB ì´ìƒ
- ë””ìŠ¤í¬: 10GB ì—¬ìœ  ê³µê°„
- OS: Windows 10/11, macOS 12+

**ê¶Œì¥ ì‚¬ì–‘:**
- RAM: 16GB ì´ìƒ
- ë””ìŠ¤í¬: 20GB ì—¬ìœ  ê³µê°„
- GPU: ì„ íƒì‚¬í•­ (Intel/AMD/NVIDIA)

---

## Ollama ì„¤ì¹˜ ë° ì„¤ì •

### Windows ì„¤ì¹˜

#### 1. Ollama ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜

```batch
# ìˆ˜ë™ ì„¤ì¹˜
1. https://ollama.com/download ë°©ë¬¸
2. Windowsìš© ì„¤ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
3. OllamaSetup.exe ì‹¤í–‰
4. ê¸°ë³¸ ê²½ë¡œë¡œ ì„¤ì¹˜ (C:\Users\[ì‚¬ìš©ì]\AppData\Local\Programs\Ollama)

# ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ (setup_ollama_windows.bat)
@echo off
chcp 65001 >nul
echo ============================================
echo Ollama ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
echo ============================================
echo.

echo [1/3] Ollama ë‹¤ìš´ë¡œë“œ ì¤‘...
curl -L https://ollama.com/download/OllamaSetup.exe -o %TEMP%\OllamaSetup.exe

echo.
echo [2/3] Ollama ì„¤ì¹˜ ì¤‘...
start /wait %TEMP%\OllamaSetup.exe /S

echo.
echo [3/3] ì„¤ì¹˜ í™•ì¸...
timeout /t 5 /nobreak >nul
ollama --version

if %errorlevel% equ 0 (
    echo.
    echo âœ“ Ollama ì„¤ì¹˜ ì™„ë£Œ!
    echo.
    echo ë‹¤ìŒ ë‹¨ê³„: setup_ollama_model.bat ì‹¤í–‰
) else (
    echo.
    echo âœ— ì„¤ì¹˜ ì‹¤íŒ¨. ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.
    echo https://ollama.com/download
)

pause
```

#### 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```batch
# setup_ollama_model.bat
@echo off
chcp 65001 >nul
echo ============================================
echo Ollama ëª¨ë¸ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
echo ============================================
echo.

echo ê¶Œì¥ ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:
echo [1] Phi-3 Mini (2.3GB) - ê°€ë³ê³  ë¹ ë¦„, ê¸°ë³¸ ë²ˆì—­
echo [2] Llama 3.2 7B (4.1GB) - ê¶Œì¥, ê³ í’ˆì§ˆ ë²ˆì—­
echo [3] Llama 3.1 8B (8.5GB) - ìµœê³  í’ˆì§ˆ
echo.

set /p choice="ì„ íƒ (1-3): "

if "%choice%"=="1" (
    set MODEL=phi3:mini
    set SIZE=2.3GB
) else if "%choice%"=="2" (
    set MODEL=llama3.2:7b-instruct-q4_K_M
    set SIZE=4.1GB
) else if "%choice%"=="3" (
    set MODEL=llama3.1:8b-instruct-q4_K_M
    set SIZE=8.5GB
) else (
    echo ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.
    pause
    exit /b 1
)

echo.
echo ì„ íƒí•œ ëª¨ë¸: %MODEL% (%SIZE%)
echo ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)
echo.

ollama pull %MODEL%

if %errorlevel% equ 0 (
    echo.
    echo âœ“ ëª¨ë¸ ì„¤ì¹˜ ì™„ë£Œ!
    echo.
    echo í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...
    echo "Hello, how are you?" | ollama run %MODEL%
    echo.
    echo í”„ë¡œê·¸ë¨ ì„¤ì •ì—ì„œ "ë¡œì»¬ LLM (Ollama)" ì„ íƒ í›„ ì‚¬ìš©í•˜ì„¸ìš”.
) else (
    echo.
    echo âœ— ëª¨ë¸ ì„¤ì¹˜ ì‹¤íŒ¨
)

pause
```

### macOS ì„¤ì¹˜

```bash
#!/bin/bash
# setup_ollama_macos.sh

echo "============================================"
echo "Ollama ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ (macOS)"
echo "============================================"
echo ""

# Homebrewë¡œ ì„¤ì¹˜
if command -v brew &> /dev/null; then
    echo "[1/2] Homebrewë¡œ Ollama ì„¤ì¹˜ ì¤‘..."
    brew install ollama
else
    echo "[1/2] Ollama ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ì¤‘..."
    curl -L https://ollama.com/download/Ollama-darwin.zip -o /tmp/Ollama.zip
    unzip /tmp/Ollama.zip -d /Applications/
fi

echo ""
echo "[2/2] Ollama ì„œë¹„ìŠ¤ ì‹œì‘..."
ollama serve &
sleep 3

echo ""
echo "âœ“ Ollama ì„¤ì¹˜ ì™„ë£Œ!"
echo ""
echo "ë‹¤ìŒ ë‹¨ê³„: ./setup_ollama_model.sh ì‹¤í–‰"
```

---

## ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

### ì¶”ì²œ ëª¨ë¸ ë¹„êµ

| ëª¨ë¸ | í¬ê¸° | RAM ì‚¬ìš© | ì†ë„ | ë²ˆì—­ í’ˆì§ˆ | ì¶”ì²œ ëŒ€ìƒ |
|------|------|----------|------|-----------|-----------|
| **Phi-3 Mini** | 2.3GB | 4-6GB | âš¡âš¡âš¡ ë¹ ë¦„ | â­â­â­ ë³´í†µ | ì €ì‚¬ì–‘, ë¹ ë¥¸ ì‘ë‹µ í•„ìš” |
| **Llama 3.2 7B** | 4.1GB | 6-8GB | âš¡âš¡ ì ë‹¹ | â­â­â­â­ ìš°ìˆ˜ | **ê¶Œì¥** ê· í˜•ì¡íŒ ì„±ëŠ¥ |
| **Llama 3.1 8B** | 8.5GB | 10-12GB | âš¡ ëŠë¦¼ | â­â­â­â­â­ ìµœê³  | ê³ ì‚¬ì–‘, ìµœê³  í’ˆì§ˆ |

### ëª¨ë¸ë³„ ì˜ˆìƒ ì„±ëŠ¥

**í…ŒìŠ¤íŠ¸ í™˜ê²½: i5-1135G7, 24GB RAM**

```
Step: ë¡œê·¸ì¸ í™”ë©´ì—ì„œ ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•œë‹¤
Expected Result: ë¡œê·¸ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ê³  ë©”ì¸ í™”ë©´ìœ¼ë¡œ ì´ë™í•œë‹¤

Phi-3 Mini (2.3GB):
- ë²ˆì—­ ì‹œê°„: ~3-5ì´ˆ
- í’ˆì§ˆ: 85%
- ì¶œë ¥:
  Step: Enter ID and password on the login screen
  Expected Result: Login is successfully completed and move to main screen

Llama 3.2 7B (4.1GB):
- ë²ˆì—­ ì‹œê°„: ~5-8ì´ˆ
- í’ˆì§ˆ: 95%
- ì¶œë ¥:
  Step: Enter ID and password on the login screen
  Expected Result: Login is successfully completed and navigates to the main screen

Llama 3.1 8B (8.5GB):
- ë²ˆì—­ ì‹œê°„: ~8-12ì´ˆ
- í’ˆì§ˆ: 98%
- ì¶œë ¥:
  Step: Enter the ID and password on the login screen
  Expected Result: Login completes successfully and navigates to the main screen
```

### Ollama ëª…ë ¹ì–´

```bash
# ëª¨ë¸ ëª©ë¡ í™•ì¸
ollama list

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull phi3:mini
ollama pull llama3.2:7b-instruct-q4_K_M
ollama pull llama3.1:8b-instruct-q4_K_M

# ëª¨ë¸ ì‚­ì œ
ollama rm phi3:mini

# ëª¨ë¸ í…ŒìŠ¤íŠ¸
ollama run llama3.2:7b-instruct-q4_K_M

# Ollama ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:11434/api/tags
```

---

## í”„ë¡œê·¸ë¨ í†µí•© êµ¬í˜„

### 1. app.py ìˆ˜ì •

#### ë²ˆì—­ í•¨ìˆ˜ ì—…ë°ì´íŠ¸

```python
# app.py

import os
import requests
from flask import Flask, request, jsonify

# ê¸°ì¡´ Gemini ê´€ë ¨ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€

def get_ollama_models():
    """Ollama ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=2)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return [model['name'] for model in models]
        return []
    except:
        return []

def check_ollama_status():
    """Ollama ì„œë²„ ì‹¤í–‰ ìƒíƒœ í™•ì¸"""
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=2)
        return response.status_code == 200
    except:
        return False

def translate_with_ollama(text, model_name='llama3.2:7b-instruct-q4_K_M', context=""):
    """Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­"""
    if not text or not isinstance(text, str) or not text.strip():
        return text
    
    prompt = f"""You are a senior QA engineer with 30 years of experience in software testing and mobile app testing. 
You are an expert in translating test cases from Korean to English while maintaining technical accuracy and clarity.

Translate the following Korean test case text to English. Keep the translation:
- Professional and technically accurate
- Clear and concise
- Using proper QA/testing terminology
- Maintaining the original meaning and intent
- Preserving line breaks and formatting

{f'Context: {context}' if context else ''}

Korean text to translate:
{text}

Provide ONLY the English translation without any additional explanation or comments."""

    try:
        logger.info(f"Translating with Ollama ({model_name}): {len(text)} chars")
        
        response = requests.post(
            'http://localhost:11434/api/generate',
            json={
                'model': model_name,
                'prompt': prompt,
                'stream': False,
                'options': {
                    'temperature': 0.3,  # ì¼ê´€ì„± ìˆëŠ” ë²ˆì—­ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
                    'top_p': 0.9,
                    'top_k': 40
                }
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            translated = result.get('response', '').strip()
            logger.info(f"Translation completed ({len(translated)} chars)")
            return translated
        else:
            error_msg = f"Ollama API error: {response.status_code}"
            logger.error(error_msg)
            if translation_status['error'] is None:
                translation_status['error'] = error_msg
            return f"[Translation Error] {text}"
            
    except Exception as e:
        import traceback
        error_msg = f"{type(e).__name__}: {str(e)}"
        logger.error(f"Translation failed - {error_msg}")
        logger.debug(traceback.format_exc())
        
        if translation_status['error'] is None:
            translation_status['error'] = error_msg
        return f"[Translation Error] {text}"

def translate_with_llm(text, context=""):
    """í†µí•© ë²ˆì—­ í•¨ìˆ˜ - ì„¤ì •ì— ë”°ë¼ Gemini ë˜ëŠ” Ollama ì‚¬ìš©"""
    
    # ë²ˆì—­ ëª¨ë“œ í™•ì¸ (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì„¤ì • íŒŒì¼ì—ì„œ)
    translation_mode = os.environ.get('TRANSLATION_MODE', 'gemini')  # 'gemini' or 'ollama'
    ollama_model = os.environ.get('OLLAMA_MODEL', 'llama3.2:7b-instruct-q4_K_M')
    
    if translation_mode == 'ollama':
        # Ollama ìƒíƒœ í™•ì¸
        if not check_ollama_status():
            error_msg = "Ollama server is not running"
            logger.error(error_msg)
            if translation_status['error'] is None:
                translation_status['error'] = error_msg
            return f"[Translation Error] {text}"
        
        return translate_with_ollama(text, ollama_model, context)
    else:
        # ê¸°ì¡´ Gemini API ì‚¬ìš©
        model = get_gemini_model()
        if not model:
            error_msg = "Gemini API model not initialized - check API key"
            logger.error(error_msg)
            return f"[Translation Error] {text}"
        
        # ê¸°ì¡´ Gemini ë²ˆì—­ ì½”ë“œ (ê·¸ëŒ€ë¡œ ìœ ì§€)
        # ... (ê¸°ì¡´ ì½”ë“œ)
```

#### API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

```python
# app.pyì— ì¶”ê°€

@app.route('/api/ollama/status', methods=['GET'])
def ollama_status():
    """Ollama ì„œë²„ ìƒíƒœ ë° ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    is_running = check_ollama_status()
    models = get_ollama_models() if is_running else []
    
    return jsonify({
        'running': is_running,
        'models': models,
        'recommended': [
            {'name': 'phi3:mini', 'size': '2.3GB', 'speed': 'fast', 'quality': 'good'},
            {'name': 'llama3.2:7b-instruct-q4_K_M', 'size': '4.1GB', 'speed': 'medium', 'quality': 'excellent'},
            {'name': 'llama3.1:8b-instruct-q4_K_M', 'size': '8.5GB', 'speed': 'slow', 'quality': 'best'}
        ]
    })

@app.route('/api/settings/translation-mode', methods=['POST'])
def set_translation_mode():
    """ë²ˆì—­ ëª¨ë“œ ì„¤ì • (gemini/ollama)"""
    data = request.json
    mode = data.get('mode', 'gemini')  # 'gemini' or 'ollama'
    model = data.get('model', 'llama3.2:7b-instruct-q4_K_M')
    
    if mode not in ['gemini', 'ollama']:
        return jsonify({'error': 'Invalid mode'}), 400
    
    # í™˜ê²½ë³€ìˆ˜ì— ì €ì¥ (ë˜ëŠ” ì„¤ì • íŒŒì¼ì— ì €ì¥)
    os.environ['TRANSLATION_MODE'] = mode
    os.environ['OLLAMA_MODEL'] = model
    
    return jsonify({
        'success': True,
        'mode': mode,
        'model': model if mode == 'ollama' else None
    })

@app.route('/api/settings/translation-mode', methods=['GET'])
def get_translation_mode():
    """í˜„ì¬ ë²ˆì—­ ëª¨ë“œ ê°€ì ¸ì˜¤ê¸°"""
    mode = os.environ.get('TRANSLATION_MODE', 'gemini')
    model = os.environ.get('OLLAMA_MODEL', 'llama3.2:7b-instruct-q4_K_M')
    
    return jsonify({
        'mode': mode,
        'model': model if mode == 'ollama' else None
    })
```

---

## ì„¤ì • í™”ë©´ êµ¬í˜„

### HTML/JavaScript ìˆ˜ì • (templates/index.html)

```html
<!-- ì„¤ì • ëª¨ë‹¬ì— ì¶”ê°€ -->

<div class="modal" id="settingsModal" style="display: none;">
    <div class="modal-content" style="max-width: 600px;">
        <h2>âš™ï¸ í™˜ê²½ ì„¤ì •</h2>
        
        <!-- ê¸°ì¡´ API Key ì„¤ì •ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ -->
        
        <!-- ë²ˆì—­ ëª¨ë“œ ì„ íƒ ì¶”ê°€ -->
        <div style="margin-top: 30px; padding-top: 20px; border-top: 1px solid #eee;">
            <h3>ğŸŒ ë²ˆì—­ ì—”ì§„ ì„ íƒ</h3>
            <p style="color: #666; font-size: 14px; margin-bottom: 15px;">
                ë¯¼ê°í•œ ë°ì´í„°ëŠ” ë¡œì»¬ LLMì„ ì‚¬ìš©í•˜ì„¸ìš” (ë„¤íŠ¸ì›Œí¬ ì „ì†¡ ì—†ìŒ)
            </p>
            
            <div class="translation-mode-selector">
                <label class="mode-option">
                    <input type="radio" name="translationMode" value="gemini" checked>
                    <div class="mode-card">
                        <div class="mode-title">ğŸŒ Gemini API</div>
                        <div class="mode-description">
                            â€¢ ë¹ ë¥¸ ì†ë„ (1-2ì´ˆ/ì…€)<br>
                            â€¢ ìµœê³  í’ˆì§ˆ<br>
                            â€¢ ì¸í„°ë„· í•„ìš”<br>
                            â€¢ API í‚¤ í•„ìš”
                        </div>
                    </div>
                </label>
                
                <label class="mode-option">
                    <input type="radio" name="translationMode" value="ollama">
                    <div class="mode-card">
                        <div class="mode-title">ğŸ”’ ë¡œì»¬ LLM (Ollama)</div>
                        <div class="mode-description">
                            â€¢ ë„¤íŠ¸ì›Œí¬ ì „ì†¡ ì—†ìŒ<br>
                            â€¢ ì™„ì „í•œ ë³´ì•ˆ/í”„ë¼ì´ë²„ì‹œ<br>
                            â€¢ ì†ë„ ëŠë¦¼ (5-10ì´ˆ/ì…€)<br>
                            â€¢ Ollama ì„¤ì¹˜ í•„ìš”
                        </div>
                        <div id="ollamaStatus" style="margin-top: 10px;">
                            <span class="status-checking">ìƒíƒœ í™•ì¸ ì¤‘...</span>
                        </div>
                    </div>
                </label>
            </div>
            
            <!-- Ollama ëª¨ë¸ ì„ íƒ (Ollama ëª¨ë“œ ì„ íƒ ì‹œì—ë§Œ í‘œì‹œ) -->
            <div id="ollamaModelSelector" style="display: none; margin-top: 20px;">
                <h4>ëª¨ë¸ ì„ íƒ</h4>
                <select id="ollamaModel" style="width: 100%; padding: 10px; border: 2px solid #667eea; border-radius: 8px;">
                    <option value="">ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...</option>
                </select>
                
                <div style="margin-top: 15px; padding: 15px; background: #f8f9ff; border-radius: 8px; font-size: 13px;">
                    <strong>ğŸ’¡ ì¶”ì²œ ëª¨ë¸:</strong><br>
                    â€¢ <strong>Llama 3.2 7B</strong> (4.1GB) - ê¶Œì¥, ê³ í’ˆì§ˆ<br>
                    â€¢ <strong>Phi-3 Mini</strong> (2.3GB) - ë¹ ë¦„, ê¸°ë³¸ ë²ˆì—­<br>
                    <br>
                    <a href="#" onclick="showOllamaInstallGuide(); return false;" style="color: #667eea;">
                        ğŸ“– Ollama ì„¤ì¹˜ ê°€ì´ë“œ ë³´ê¸°
                    </a>
                </div>
            </div>
        </div>
        
        <div style="display: flex; gap: 10px; margin-top: 30px;">
            <button id="saveSettingsBtn" class="btn-primary">ì €ì¥</button>
            <button onclick="closeSettingsModal()" class="btn-secondary">ì·¨ì†Œ</button>
        </div>
    </div>
</div>

<!-- Ollama ì„¤ì¹˜ ê°€ì´ë“œ ëª¨ë‹¬ -->
<div class="modal" id="ollamaGuideModal" style="display: none;">
    <div class="modal-content" style="max-width: 700px;">
        <h2>ğŸ“– Ollama ì„¤ì¹˜ ê°€ì´ë“œ</h2>
        
        <div style="text-align: left; line-height: 1.8;">
            <h3>Windows ì„¤ì¹˜</h3>
            <ol>
                <li><a href="https://ollama.com/download" target="_blank">Ollama ë‹¤ìš´ë¡œë“œ í˜ì´ì§€</a> ë°©ë¬¸</li>
                <li>Windowsìš© ì„¤ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ</li>
                <li>ì„¤ì¹˜ í›„ ëª…ë ¹ í”„ë¡¬í”„íŠ¸(CMD) ì‹¤í–‰</li>
                <li>ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:<br>
                    <code style="background: #f0f0f0; padding: 5px 10px; border-radius: 4px; display: inline-block; margin-top: 5px;">
                        ollama pull llama3.2:7b-instruct-q4_K_M
                    </code>
                </li>
                <li>ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ì´ í”„ë¡œê·¸ë¨ì—ì„œ "ë¡œì»¬ LLM" ì„ íƒ</li>
            </ol>
            
            <h3>macOS ì„¤ì¹˜</h3>
            <ol>
                <li>í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰:<br>
                    <code style="background: #f0f0f0; padding: 5px 10px; border-radius: 4px; display: inline-block; margin-top: 5px;">
                        brew install ollama
                    </code>
                </li>
                <li>ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:<br>
                    <code style="background: #f0f0f0; padding: 5px 10px; border-radius: 4px; display: inline-block; margin-top: 5px;">
                        ollama pull llama3.2:7b-instruct-q4_K_M
                    </code>
                </li>
            </ol>
        </div>
        
        <button onclick="closeOllamaGuideModal()" class="btn-primary" style="margin-top: 20px;">ë‹«ê¸°</button>
    </div>
</div>

<style>
.translation-mode-selector {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 15px;
    margin-bottom: 20px;
}

.mode-option {
    cursor: pointer;
}

.mode-option input[type="radio"] {
    display: none;
}

.mode-card {
    padding: 20px;
    border: 2px solid #ddd;
    border-radius: 12px;
    transition: all 0.3s ease;
    background: white;
}

.mode-option input[type="radio"]:checked + .mode-card {
    border-color: #667eea;
    background: #f8f9ff;
    box-shadow: 0 4px 12px rgba(102, 126, 234, 0.2);
}

.mode-title {
    font-size: 16px;
    font-weight: bold;
    margin-bottom: 10px;
    color: #333;
}

.mode-description {
    font-size: 13px;
    color: #666;
    line-height: 1.6;
}

.status-checking {
    color: #999;
    font-size: 12px;
}

.status-running {
    color: #28a745;
    font-size: 12px;
    font-weight: bold;
}

.status-not-running {
    color: #dc3545;
    font-size: 12px;
    font-weight: bold;
}

code {
    font-family: 'Courier New', monospace;
    font-size: 13px;
}
</style>

<script>
// ì„¤ì • ëª¨ë‹¬ ì—´ ë•Œ Ollama ìƒíƒœ í™•ì¸
async function openSettingsModal() {
    document.getElementById('settingsModal').style.display = 'flex';
    
    // ê¸°ì¡´ API í‚¤ ê´€ë ¨ ì½”ë“œ...
    
    // Ollama ìƒíƒœ í™•ì¸
    await checkOllamaStatus();
    
    // í˜„ì¬ ë²ˆì—­ ëª¨ë“œ ë¶ˆëŸ¬ì˜¤ê¸°
    await loadTranslationMode();
}

async function checkOllamaStatus() {
    try {
        const response = await fetch('/api/ollama/status');
        const data = await response.json();
        
        const statusDiv = document.getElementById('ollamaStatus');
        
        if (data.running) {
            statusDiv.innerHTML = `
                <span class="status-running">âœ“ Ollama ì‹¤í–‰ ì¤‘</span>
                <span style="color: #666; font-size: 11px; margin-left: 10px;">
                    (ëª¨ë¸ ${data.models.length}ê°œ ì‚¬ìš© ê°€ëŠ¥)
                </span>
            `;
            
            // ëª¨ë¸ ëª©ë¡ ì—…ë°ì´íŠ¸
            const modelSelect = document.getElementById('ollamaModel');
            modelSelect.innerHTML = data.models.map(model => 
                `<option value="${model}">${model}</option>`
            ).join('');
            
            // ê¶Œì¥ ëª¨ë¸ ì¶”ê°€ (ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°)
            if (data.models.length === 0) {
                modelSelect.innerHTML = data.recommended.map(model =>
                    `<option value="${model.name}">${model.name} (${model.size}) - ${model.quality}</option>`
                ).join('');
            }
        } else {
            statusDiv.innerHTML = `
                <span class="status-not-running">âœ— Ollamaê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤</span>
                <a href="#" onclick="showOllamaInstallGuide(); return false;" 
                   style="color: #667eea; font-size: 11px; margin-left: 10px;">
                    ì„¤ì¹˜ ê°€ì´ë“œ
                </a>
            `;
        }
    } catch (error) {
        console.error('Ollama ìƒíƒœ í™•ì¸ ì‹¤íŒ¨:', error);
        document.getElementById('ollamaStatus').innerHTML = 
            '<span class="status-not-running">âœ— ìƒíƒœ í™•ì¸ ì‹¤íŒ¨</span>';
    }
}

async function loadTranslationMode() {
    try {
        const response = await fetch('/api/settings/translation-mode');
        const data = await response.json();
        
        // ë¼ë””ì˜¤ ë²„íŠ¼ ì„ íƒ
        document.querySelector(`input[name="translationMode"][value="${data.mode}"]`).checked = true;
        
        // Ollama ëª¨ë“œë©´ ëª¨ë¸ ì„ íƒ í‘œì‹œ
        if (data.mode === 'ollama') {
            document.getElementById('ollamaModelSelector').style.display = 'block';
            if (data.model) {
                document.getElementById('ollamaModel').value = data.model;
            }
        }
    } catch (error) {
        console.error('ë²ˆì—­ ëª¨ë“œ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨:', error);
    }
}

// ë²ˆì—­ ëª¨ë“œ ë¼ë””ì˜¤ ë²„íŠ¼ ë³€ê²½ ì´ë²¤íŠ¸
document.querySelectorAll('input[name="translationMode"]').forEach(radio => {
    radio.addEventListener('change', (e) => {
        const isOllama = e.target.value === 'ollama';
        document.getElementById('ollamaModelSelector').style.display = isOllama ? 'block' : 'none';
    });
});

// ì„¤ì • ì €ì¥
document.getElementById('saveSettingsBtn').addEventListener('click', async () => {
    const mode = document.querySelector('input[name="translationMode"]:checked').value;
    const model = document.getElementById('ollamaModel').value;
    
    try {
        // ê¸°ì¡´ API í‚¤ ì €ì¥ ì½”ë“œ...
        
        // ë²ˆì—­ ëª¨ë“œ ì €ì¥
        const response = await fetch('/api/settings/translation-mode', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ mode, model })
        });
        
        if (response.ok) {
            alert('ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.');
            closeSettingsModal();
        } else {
            alert('ì„¤ì • ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
        }
    } catch (error) {
        console.error('ì„¤ì • ì €ì¥ ì‹¤íŒ¨:', error);
        alert('ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    }
});

function showOllamaInstallGuide() {
    document.getElementById('ollamaGuideModal').style.display = 'flex';
}

function closeOllamaGuideModal() {
    document.getElementById('ollamaGuideModal').style.display = 'none';
}
</script>
```

---

## ë°°í¬ ë° ì‚¬ìš©ì ê°€ì´ë“œ

### ê´€ë¦¬ììš© ë°°í¬ ê°€ì´ë“œ

#### 1. ì „ì²´ PC ì¼ê´„ ì„¤ì¹˜

```batch
REM deploy_ollama_all.bat
@echo off
chcp 65001 >nul
echo ============================================
echo Ollama ì¼ê´„ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
echo ============================================
echo.

REM ë„¤íŠ¸ì›Œí¬ ê³µìœ  í´ë” ê²½ë¡œ (ì„¤ì¹˜ íŒŒì¼ ìœ„ì¹˜)
set SHARE_PATH=\\server\share\ollama

echo [1/3] Ollama ì„¤ì¹˜...
start /wait %SHARE_PATH%\OllamaSetup.exe /S

echo.
echo [2/3] ê¸°ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (Llama 3.2 7B)...
ollama pull llama3.2:7b-instruct-q4_K_M

echo.
echo [3/3] ìë™ ì‹œì‘ ì„¤ì •...
reg add "HKCU\Software\Microsoft\Windows\CurrentVersion\Run" /v "Ollama" /t REG_SZ /d "%LOCALAPPDATA%\Programs\Ollama\ollama.exe serve" /f

echo.
echo âœ“ ë°°í¬ ì™„ë£Œ!
pause
```

#### 2. ì‚¬ìš©ìë³„ ê°€ì´ë“œ ë¬¸ì„œ

```markdown
# ğŸ“˜ TestGPT TC Translator - ë¡œì»¬ LLM ì‚¬ìš© ê°€ì´ë“œ

## Ollama ì„¤ì¹˜ í™•ì¸

1. ì‹œì‘ ë©”ë‰´ì—ì„œ "CMD" ë˜ëŠ” "ëª…ë ¹ í”„ë¡¬í”„íŠ¸" ê²€ìƒ‰
2. ë‹¤ìŒ ëª…ë ¹ì–´ ì…ë ¥:
   ```
   ollama --version
   ```
3. ë²„ì „ì´ í‘œì‹œë˜ë©´ ì„¤ì¹˜ ì™„ë£Œ

## í”„ë¡œê·¸ë¨ ì„¤ì •

1. TestGPT TC Translator ì‹¤í–‰
2. ìš°ì¸¡ ìƒë‹¨ **âš™ï¸ í™˜ê²½ì„¤ì •** ë²„íŠ¼ í´ë¦­
3. "ë²ˆì—­ ì—”ì§„ ì„ íƒ" ì„¹ì…˜ì—ì„œ **ğŸ”’ ë¡œì»¬ LLM (Ollama)** ì„ íƒ
4. "ëª¨ë¸ ì„ íƒ"ì—ì„œ **llama3.2:7b-instruct-q4_K_M** ì„ íƒ
5. **ì €ì¥** ë²„íŠ¼ í´ë¦­

## ì‚¬ìš© ë°©ë²•

- ì¼ë°˜ ë°ì´í„°: Gemini API (ë¹ ë¦„)
- ë¯¼ê°í•œ ë°ì´í„°: ë¡œì»¬ LLM (ë³´ì•ˆ)

ì„¤ì •ì—ì„œ ì–¸ì œë“ ì§€ ë³€ê²½ ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ë¬¸ì œ í•´ê²°

**Q: "Ollamaê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤" ì˜¤ë¥˜**
A: ì‹œì‘ ë©”ë‰´ì—ì„œ "Ollama" ê²€ìƒ‰ í›„ ì‹¤í–‰

**Q: ë²ˆì—­ì´ ë„ˆë¬´ ëŠë ¤ìš”**
A: ë¡œì»¬ LLMì€ ë³´ì•ˆì„ ìœ„í•´ ì†ë„ë¥¼ í¬ìƒí•©ë‹ˆë‹¤. ë¹ ë¥¸ ë²ˆì—­ì´ í•„ìš”í•˜ë©´ Gemini APIë¡œ ì „í™˜í•˜ì„¸ìš”.

**Q: ëª¨ë¸ì´ ì—†ë‹¤ê³  ë‚˜ì™€ìš”**
A: ëª…ë ¹ í”„ë¡¬í”„íŠ¸ì—ì„œ ë‹¤ìŒ ì‹¤í–‰:
   ```
   ollama pull llama3.2:7b-instruct-q4_K_M
   ```
```

### ìë™í™”ëœ ì„¤ì¹˜ íŒ¨í‚¤ì§€ ìƒì„±

```batch
REM create_deployment_package.bat
@echo off
echo ë°°í¬ íŒ¨í‚¤ì§€ ìƒì„± ì¤‘...

REM ë°°í¬ í´ë” ìƒì„±
mkdir deployment
mkdir deployment\scripts
mkdir deployment\guides

REM ìŠ¤í¬ë¦½íŠ¸ ë³µì‚¬
copy setup_ollama_windows.bat deployment\scripts\
copy setup_ollama_model.bat deployment\scripts\

REM ê°€ì´ë“œ ë¬¸ì„œ ë³µì‚¬
copy USER_GUIDE.md deployment\guides\
copy TROUBLESHOOTING.md deployment\guides\

REM Ollama ì„¤ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­)
echo Ollama ì„¤ì¹˜ íŒŒì¼ì„ deployment\ í´ë”ì— ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€í•˜ì„¸ìš”.
echo https://ollama.com/download

echo.
echo âœ“ ë°°í¬ íŒ¨í‚¤ì§€ ì¤€ë¹„ ì™„ë£Œ: deployment\ í´ë”
pause
```

---

## ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

#### 1. Ollamaê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ:** "Ollama server is not running" ì˜¤ë¥˜

**í•´ê²°:**
```batch
# Windows
1. ì‘ì—… ê´€ë¦¬ì ì‹¤í–‰ (Ctrl + Shift + Esc)
2. "ollama.exe" í”„ë¡œì„¸ìŠ¤ í™•ì¸
3. ì—†ìœ¼ë©´ ì‹œì‘ ë©”ë‰´ì—ì„œ "Ollama" ê²€ìƒ‰ í›„ ì‹¤í–‰

# ë˜ëŠ” ëª…ë ¹ í”„ë¡¬í”„íŠ¸ì—ì„œ
ollama serve

# macOS
brew services start ollama
```

#### 2. ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ

**ì¦ìƒ:** "Model not found" ì˜¤ë¥˜

**í•´ê²°:**
```bash
# ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
ollama list

# ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.2:7b-instruct-q4_K_M
```

#### 3. ë²ˆì—­ì´ ë§¤ìš° ëŠë¦¼

**ì›ì¸:** CPUë§Œ ì‚¬ìš© ì¤‘

**í•´ê²°:**
- GPU ë“œë¼ì´ë²„ ìµœì‹  ë²„ì „ ì„¤ì¹˜
- OllamaëŠ” ìë™ìœ¼ë¡œ GPU ê°ì§€ ë° ì‚¬ìš©
- í™•ì¸: ì‘ì—… ê´€ë¦¬ì > ì„±ëŠ¥ > GPU ì‚¬ìš©ë¥  í™•ì¸

#### 4. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜

**ì¦ìƒ:** "Out of memory" ì˜¤ë¥˜

**í•´ê²°:**
```bash
# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
ollama pull phi3:mini  # 2.3GB

# í”„ë¡œê·¸ë¨ ì„¤ì •ì—ì„œ Phi-3 Mini ì„ íƒ
```

#### 5. ì—°ê²° ì‹¤íŒ¨ (Connection refused)

**ì¦ìƒ:** "Failed to connect to Ollama"

**í•´ê²°:**
```bash
# Ollama ì„œë²„ ì¬ì‹œì‘
# Windows
taskkill /IM ollama.exe /F
ollama serve

# macOS
brew services restart ollama
```

### ë¡œê·¸ í™•ì¸

```batch
# Windows
echo %LOCALAPPDATA%\Ollama\logs

# macOS
~/Library/Logs/Ollama
```

### ì„±ëŠ¥ ìµœì í™”

```bash
# GPU ë©”ëª¨ë¦¬ ì„¤ì • (ì„ íƒì‚¬í•­)
# Linux/macOS: ~/.ollama/config.json
{
  "gpu_memory_fraction": 0.8,  # GPU ë©”ëª¨ë¦¬ 80% ì‚¬ìš©
  "num_threads": 8             # CPU ìŠ¤ë ˆë“œ ìˆ˜
}
```

---

## ì¶”ê°€ ì°¸ê³  ìë£Œ

### Ollama ê³µì‹ ë¬¸ì„œ
- ì›¹ì‚¬ì´íŠ¸: https://ollama.com
- GitHub: https://github.com/ollama/ollama
- ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬: https://ollama.com/library

### ê¶Œì¥ ëª¨ë¸ ìƒì„¸ ì •ë³´

**Phi-3 Mini**
- ê°œë°œ: Microsoft
- íŒŒë¼ë¯¸í„°: 3.8B
- íŠ¹ì§•: ë§¤ìš° ë¹ ë¥´ê³  ê°€ë²¼ì›€
- ìš©ë„: ê°„ë‹¨í•œ ë²ˆì—­, ì €ì‚¬ì–‘ PC

**Llama 3.2 7B**
- ê°œë°œ: Meta
- íŒŒë¼ë¯¸í„°: 7B
- íŠ¹ì§•: ê· í˜•ì¡íŒ ì„±ëŠ¥
- ìš©ë„: ì¼ë°˜ì ì¸ ë²ˆì—­ ì‘ì—…

**Llama 3.1 8B**
- ê°œë°œ: Meta  
- íŒŒë¼ë¯¸í„°: 8B
- íŠ¹ì§•: ìµœê³  í’ˆì§ˆ
- ìš©ë„: ê³ í’ˆì§ˆ ë²ˆì—­ í•„ìš” ì‹œ

### API ëª…ë ¹ì–´ ì°¸ê³ 

```bash
# ëª¨ë¸ ì •ë³´ í™•ì¸
curl http://localhost:11434/api/show -d '{
  "name": "llama3.2:7b-instruct-q4_K_M"
}'

# ë²ˆì—­ í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:7b-instruct-q4_K_M",
  "prompt": "Translate to English: ë¡œê·¸ì¸ í™”ë©´",
  "stream": false
}'

# ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸
curl http://localhost:11434/api/ps
```

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°°í¬ ì „ í™•ì¸ì‚¬í•­

- [ ] Ollama ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ í…ŒìŠ¤íŠ¸
- [ ] ê¶Œì¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸
- [ ] í”„ë¡œê·¸ë¨ ì„¤ì • í™”ë©´ í…ŒìŠ¤íŠ¸
- [ ] Gemini â†” Ollama ì „í™˜ í…ŒìŠ¤íŠ¸
- [ ] ë²ˆì—­ í’ˆì§ˆ ë¹„êµ í…ŒìŠ¤íŠ¸
- [ ] ì‚¬ìš©ì ê°€ì´ë“œ ë¬¸ì„œ ì‘ì„±
- [ ] ë¬¸ì œ í•´ê²° ê°€ì´ë“œ ì‘ì„±

### ì‚¬ìš©ì êµìœ¡ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Ollama ì„¤ì¹˜ ë°©ë²• ì„¤ëª…
- [ ] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë°©ë²• ì„¤ëª…
- [ ] í”„ë¡œê·¸ë¨ ì„¤ì • ë°©ë²• ì„¤ëª…
- [ ] ë²ˆì—­ ëª¨ë“œ ì„ íƒ ê¸°ì¤€ ì„¤ëª…
- [ ] ë¬¸ì œ ë°œìƒ ì‹œ ëŒ€ì‘ ë°©ë²• ì„¤ëª…

---

## ë²„ì „ ê´€ë¦¬

- **v1.0** (2026-02-02): ì´ˆê¸° ê°€ì´ë“œ ì‘ì„±
- **v1.1** (ì˜ˆì •): ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
- **v1.2** (ì˜ˆì •): macOS ì§€ì› ê°•í™”

---

**ì‘ì„±ì:** GitHub Copilot  
**ì‘ì„±ì¼:** 2026ë…„ 2ì›” 2ì¼  
**ë¬¸ì„œ ëª©ì :** ë‚˜ì¤‘ì— ë‹¤ë¥¸ í™˜ê²½ì—ì„œ Ollama ë¡œì»¬ LLM í†µí•©ì„ ìœ„í•œ ì™„ì „í•œ ì°¸ê³  ìë£Œ
